{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gltmkm506/alex.us/blob/main/Twitch_Classifier_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZFoTZ9Rd4bP"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp22/blob/master/HW2/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQTT9x-6d2JI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64edf4e4-b1f9-4594-8e1e-969f9032c04f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import sys, argparse\n",
        "from scipy import sparse\n",
        "from scipy.stats import norm\n",
        "from sklearn import linear_model\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import operator\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "import csv\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pandas import option_context\n",
        "import math\n",
        "import requests\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4KuVSCSqlUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "949ace2a-ff47-4a2b-a4cc-c17b8409a07f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader punkt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import Orthrus Lexicon's Toxic Dictionary\n",
        "toxic_url = \"https://raw.githubusercontent.com/Orthrus-Lexicon/Toxic/main/Toxic%20words%20dictionary.txt\"\n",
        "toxic_dict = requests.get(toxic_url).text.split()"
      ],
      "metadata": {
        "id": "mjJRQXcYzRgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(toxic_dict), toxic_dict[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4oVJhQD1Ocs",
        "outputId": "8912db02-b75a-44aa-fb30-1bca95f09169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> ['***', '*itches', '4r5e', '5h1t', '5hit', 'God', 'God', 'damn', 'Goddamn', 'a**']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq2yq0xpRCUb"
      },
      "outputs": [],
      "source": [
        "trainingFile = \"/content/drive/MyDrive/ap_data/ap_data/37/train.txt\"\n",
        "evaluationFile = \"/content/drive/MyDrive/ap_data/ap_data/37/dev.txt\"\n",
        "testFile = \"/content/drive/MyDrive/ap_data/ap_data/37/test.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGiM8qQiJOBU"
      },
      "outputs": [],
      "source": [
        "class Classifier:\n",
        "\n",
        "    def __init__(self, feature_method, L2_regularization_strength=1.0, min_feature_count=1):\n",
        "        self.feature_vocab = {}\n",
        "        self.feature_method = feature_method\n",
        "        self.log_reg = None\n",
        "        self.L2_regularization_strength=L2_regularization_strength\n",
        "        self.min_feature_count=min_feature_count\n",
        "\n",
        "        self.trainX, self.trainY, self.trainOrig = self.process(trainingFile, training=True)\n",
        "        self.devX, self.devY, self.devOrig = self.process(evaluationFile, training=False)\n",
        "        self.testX, _, self.testOrig = self.process(testFile, training=False)\n",
        "\n",
        "    # Read data from file\n",
        "    def load_data(self, filename):\n",
        "        data = []\n",
        "        with open(filename, encoding=\"utf8\") as file:\n",
        "            for line in file:\n",
        "                cols = line.split(\"\\t\")\n",
        "                idd = cols[0]\n",
        "                label = cols[1]\n",
        "                text = cols[2]\n",
        "\n",
        "                data.append((idd, label, text))\n",
        "                \n",
        "        return data\n",
        "\n",
        "    # Featurize entire dataset\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for idd, label, text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append((label, feats))\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, dataFile, training = False):\n",
        "        original_data = self.load_data(dataFile)\n",
        "        data = self.featurize(original_data)\n",
        "\n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for label, feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (label, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = label\n",
        "        if training:\n",
        "          X, Y = SMOTE().fit_resample(X, Y)\n",
        "\n",
        "        return X, Y, original_data\n",
        "\n",
        "    def load_test(self, dataFile):\n",
        "        data = self.load_data(dataFile)\n",
        "        data = self.featurize(data)\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (data_id, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = data_id\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    # Train model and evaluate on held-out data\n",
        "    def evaluate(self):\n",
        "        (D,F) = self.trainX.shape\n",
        "        self.log_reg = linear_model.LogisticRegression(C = self.L2_regularization_strength, max_iter=1000)\n",
        "        self.log_reg.fit(self.trainX, self.trainY)\n",
        "        training_accuracy = self.log_reg.score(self.trainX, self.trainY)\n",
        "        development_accuracy = self.log_reg.score(self.devX, self.devY)\n",
        "        print(\"Method: %s, Features: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (self.feature_method.__name__, F, training_accuracy, development_accuracy))\n",
        "\n",
        "\n",
        "    # Predict labels for new data\n",
        "    def predict(self):\n",
        "        predX = self.log_reg.predict(self.testX)\n",
        "\n",
        "        with open(\"%s_%s\" % (self.feature_method.__name__, \"predictions.csv\"), \"w\", encoding=\"utf8\") as out:\n",
        "            writer=csv.writer(out)\n",
        "            writer.writerow([\"Id\", \"Expected\"])\n",
        "            for idx, data_id in enumerate(self.testX):\n",
        "                writer.writerow([self.testOrig[idx][0], predX[idx]])\n",
        "        out.close()\n",
        "\n",
        "\n",
        "    def printWeights(self, n=10):\n",
        "\n",
        "        reverse_vocab=[None]*len(self.log_reg.coef_[0])\n",
        "        for k in self.feature_vocab:\n",
        "            reverse_vocab[self.feature_vocab[k]]=k\n",
        "\n",
        "        # binary\n",
        "        if len(self.log_reg.classes_) == 2:\n",
        "              weights=self.log_reg.coef_[0]\n",
        "\n",
        "              cat=self.log_reg.classes_[1]\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "              cat=self.log_reg.classes_[0]\n",
        "              for feature, weight in list(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1)))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "        # multiclass\n",
        "        else:\n",
        "          for i, cat in enumerate(self.log_reg.classes_):\n",
        "\n",
        "              weights=self.log_reg.coef_[i]\n",
        "\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCq1bL3e2jUj"
      },
      "outputs": [],
      "source": [
        "def orthrus_feat(text):\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "        if word in toxic_dict:\n",
        "            feats[\"contains_toxic_word\"] = 1\n",
        "            \n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jnqjxd6fKPiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4310e8c-96f1-490a-d64c-f532fee2f72c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: orthrus_feat, Features: 1, Train accuracy: 0.435, Dev accuracy: 0.830\n"
          ]
        }
      ],
      "source": [
        "simple_classifier = Classifier(orthrus_feat, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "simple_classifier.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def wn_feat(text):\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "        for sn in wn.synsets(word):\n",
        "          feats[sn] = 1\n",
        "            \n",
        "    return feats"
      ],
      "metadata": {
        "id": "2VGRMZj8QOc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifierwn = Classifier(wn_feat, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifierwn.evaluate()"
      ],
      "metadata": {
        "id": "32102Qsj1AC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a35422-c029-4fbe-e0e0-15301ebce1fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: wn_feat, Features: 5095, Train accuracy: 0.940, Dev accuracy: 0.830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNlQyjEB4Bwt"
      },
      "outputs": [],
      "source": [
        "def bigram_feat(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "    feats = {}\n",
        "\n",
        "    bigrams = nltk.bigrams(text)\n",
        "    for bigram in bigrams:\n",
        "        feats[bigram] = 1\n",
        "    \n",
        "            \n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram_feat2(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for i in range(0, len(words) - 1):\n",
        "      feats[words[i] + \" \" + words[i+1]] = 1\n",
        "               \n",
        "    return feats"
      ],
      "metadata": {
        "id": "ll40Lop3uphk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgpuykF67oWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0633a68-f613-40f8-8f80-d0631fcef80b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: bigram_feat, Features: 1537, Train accuracy: 0.999, Dev accuracy: 0.835\n"
          ]
        }
      ],
      "source": [
        "classifier2 = Classifier(bigram_feat, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier2.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmJKucgn4CEg"
      },
      "outputs": [],
      "source": [
        "def stem_feat(text):\n",
        "    \n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "    ps = nltk.stem.PorterStemmer()\n",
        "    for word in words:\n",
        "      stem = ps.stem(word)\n",
        "      feats[stem]=1\n",
        "\n",
        "    \n",
        "            \n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_f--utb7q4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b0313e-98d8-4148-bfaa-06be591c125b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: stem_feat, Features: 1279, Train accuracy: 0.999, Dev accuracy: 0.875\n"
          ]
        }
      ],
      "source": [
        "classifier3 = Classifier(stem_feat, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier3.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "    for word in words:\n",
        "      feats[word] = 1\n",
        "\n",
        "    return feats"
      ],
      "metadata": {
        "id": "4aLhXeMpyDdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_classifier = Classifier(bag_of_words, L2_regularization_strength=0.75, min_feature_count=1)\n",
        "bow_classifier.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj_aaQYYyEoJ",
        "outputId": "ce0cf1f4-47c8-4eb4-d4bc-4cc89fcbd6c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: bag_of_words, Features: 1447, Train accuracy: 1.000, Dev accuracy: 0.885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEpK5LyMgv5c"
      },
      "source": [
        "Next, let's combine any or all the features we have developed into one big model and make predictions on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxKmEqI5JY71"
      },
      "outputs": [],
      "source": [
        "def combiner_function(text):\n",
        "\n",
        "    # Here the `all_feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "  all_feats={}\n",
        "  for feature in [orthrus_feat, wn_feat, bigram_feat2, stem_feat]:\n",
        "    all_feats.update(feature(text))\n",
        "  return all_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-tRUFTIdAqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "670cf951-58f5-4eee-da71-6a5659e6eecf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:811: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  args=(X, target, 1.0 / C, sample_weight),\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:301: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  loss += 0.5 * alpha * squared_norm(w)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:352: RuntimeWarning: invalid value encountered in multiply\n",
            "  grad[:, :n_features] += alpha * w\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
            "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: combiner_function, Features: 8643, Train accuracy: 0.333, Dev accuracy: 0.915\n",
            "0.01\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.945, Dev accuracy: 0.870\n",
            "0.02\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.978, Dev accuracy: 0.875\n",
            "0.03\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.986, Dev accuracy: 0.880\n",
            "0.04\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.993, Dev accuracy: 0.880\n",
            "0.05\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.995, Dev accuracy: 0.885\n",
            "0.06\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.995, Dev accuracy: 0.890\n",
            "0.07\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.997, Dev accuracy: 0.890\n",
            "0.08\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.997, Dev accuracy: 0.895\n",
            "0.09\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.997, Dev accuracy: 0.895\n",
            "0.1\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.997, Dev accuracy: 0.905\n",
            "0.11\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.997, Dev accuracy: 0.900\n",
            "0.12\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.997, Dev accuracy: 0.895\n",
            "0.13\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.997, Dev accuracy: 0.905\n",
            "0.14\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.998, Dev accuracy: 0.895\n",
            "0.15\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.998, Dev accuracy: 0.900\n",
            "0.16\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.999, Dev accuracy: 0.890\n",
            "0.17\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.999, Dev accuracy: 0.895\n",
            "0.18\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.998, Dev accuracy: 0.895\n",
            "0.19\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.999, Dev accuracy: 0.895\n",
            "0.2\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.999, Dev accuracy: 0.900\n",
            "0.21\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.999, Dev accuracy: 0.890\n",
            "0.22\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.999, Dev accuracy: 0.895\n",
            "0.23\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.999, Dev accuracy: 0.890\n",
            "0.24\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.999, Dev accuracy: 0.895\n",
            "0.25\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.999, Dev accuracy: 0.895\n",
            "0.26\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.999, Dev accuracy: 0.895\n",
            "0.27\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.999, Dev accuracy: 0.900\n",
            "0.28\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.999, Dev accuracy: 0.890\n",
            "0.29\n",
            "Method: combiner_function, Features: 8643, Train accuracy: 0.999, Dev accuracy: 0.890\n"
          ]
        }
      ],
      "source": [
        "# testing l2 regularization strength value to find value with best dev accuracy; found to be 0.0\n",
        "for i in np.arange(0,0.3,0.01):\n",
        "  big_classifier = Classifier(combiner_function, L2_regularization_strength=i, min_feature_count=1)\n",
        "  print(i)\n",
        "  big_classifier.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using 0.0 as L2_regularization_strength weight throwing error\n",
        "big_classifier = Classifier(combiner_function, L2_regularization_strength=1.0, min_feature_count=1)  \n",
        "big_classifier.evaluate()\n",
        "big_classifier.predict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP9tOJ6H3bHT",
        "outputId": "bc83831b-a89c-44ba-f36c-d4a47f820743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: combiner_function, Features: 8643, Train accuracy: 0.999, Dev accuracy: 0.890\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def confidence_intervals(accuracy, n, significance_level):\n",
        "    critical_value=(1-significance_level)/2\n",
        "    z_alpha=-1*norm.ppf(critical_value)\n",
        "    se=math.sqrt((accuracy*(1-accuracy))/n)\n",
        "    return accuracy-(se*z_alpha), accuracy+(se*z_alpha)"
      ],
      "metadata": {
        "id": "Ofz_NIFouH7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower, upper=confidence_intervals(0.915, 200, .95)\n",
        "print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (0.915, lower, upper))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWKcmA9GuJ8B",
        "outputId": "47f3cda7-c41e-413c-a698-d0f131adf02f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy for best dev model: 0.915, 95% CIs: [0.876 0.954]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg2J1BLgatMP"
      },
      "source": [
        " ## Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the biggest challenges with our attempt to classify toxic twitch chat messages is the large class inbalance. With roughly 92% of the messages in the dataset being \"not toxic\" and only 1000 messages in total, the scarcity of \"toxic\" and \"unclear\" messages make it very hard for our classifier to learn what makes a message \"toxic\" or \"unclear\". Additionally, taking into account the wide range of ways that a message could be toxic, there were likely many new forms of toxicity that were unseen during training. For example, one form of toxicity would be messages that include threats to harm a person or group of people. When annotating, we saw very few examples of messages that fell into this category and thus our classifier likely struggled to identify these forms of toxicity. Despite our classifier falling short of being able to accurately label toxic messages (final dev accuracy was equal to both the majority class and BOW classifiers), let us dig deeper into what was our classifier was able and not able to decipher regarding toxic messages."
      ],
      "metadata": {
        "id": "TjO_DW7q5y2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the confusion matrix below for our final classifier, we can see that our classifier is not doing great at indentifying \"toxic\" and \"unclear\" messages. Out of the 9 toxic messages in the dev set, our classifier was only able to correctly label 3 of the messages as toxic. The performance was even worse for \"unclear\" messages, with our classifier correctly identifying 0 out of 8. Again this is likely highly attributed to the class inbalance, which we can also see through the confusion matrix as 183 out of the 200 messages in the dev set had a true label of \"not toxic\"."
      ],
      "metadata": {
        "id": "R9Ct4I02CzI3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ulxd1TosIMV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "cf40d108-5311-4664-ae86-5f28f6a9d7fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "def print_confusion(classifier):\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    plot_confusion_matrix(classifier.log_reg, classifier.devX, classifier.devY, ax=ax, xticks_rotation=\"vertical\", values_format=\"d\")\n",
        "    plt.show()\n",
        "\n",
        "print_confusion(big_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let us look at the top coefficient weights for each class to understand which features are being considered the most heavily in determining the label. The one feature that appears to be performing as intended is the contain_toxic_word feature, which is a binary label that indicates whether a message contains a toxic word from the Orthrus Lexicon's Toxic Dictionary, since the feature has a high weight for toxic labels and a slighly smaller but still significant weight for unclear labels. Another success in regards to the features is that we were hoping to capture toxic phrases like \"u suck\" or \"f**k you\", which appeared to work based on the feature coefficients. However, for the other features used like word sense and the word stems, the feature weights seem to fairly random and doesn't provide much insight on how the model is classifying messages; they were not accurate in reflecting label boundaries. "
      ],
      "metadata": {
        "id": "rzv57F8nCv8D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAyGuXIi9pqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b273c47d-a89e-4e90-940a-bb3748f60cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N\t0.578\tkennen\n",
            "N\t0.578\tkennen carry\n",
            "N\t0.576\twtf\n",
            "N\t0.500\t!\n",
            "N\t0.469\tlmfao\n",
            "N\t0.467\tpogger\n",
            "N\t0.463\t@\n",
            "N\t0.360\tomegalul\n",
            "N\t0.334\tSynset('carry.n.01')\n",
            "N\t0.325\tholi\n",
            "N\t0.325\tSynset('holy.a.01')\n",
            "N\t0.325\tSynset('holy_place.n.01')\n",
            "N\t0.308\tjesu\n",
            "N\t0.308\tJesus fuck\n",
            "N\t0.308\tSynset('jesus.n.01')\n",
            "N\t0.307\tU\n",
            "N\t0.307\tLOVE U\n",
            "N\t0.303\tit\n",
            "N\t0.289\tninja\n",
            "N\t0.289\tSynset('ninja.n.02')\n",
            "N\t0.289\tSynset('ninja.n.01')\n",
            "N\t0.274\tSynset('information_technology.n.01')\n",
            "N\t0.241\tSynset('hello.n.01')\n",
            "N\t0.230\tthat\n",
            "N\t0.226\tholy fuck\n",
            "\n",
            "T\t3.424\tpussyyyy\n",
            "T\t1.130\tmidget\n",
            "T\t1.130\tSynset('bantam.s.01')\n",
            "T\t1.111\tSynset('dwarf.n.01')\n",
            "T\t0.917\tcontains_toxic_word\n",
            "T\t0.745\tyousif\n",
            "T\t0.745\tYOU YOUSIF\n",
            "T\t0.745\tFUCK YOU\n",
            "T\t0.717\tyou\n",
            "T\t0.702\tsuc\n",
            "T\t0.702\tu suc\n",
            "T\t0.669\tfranc\n",
            "T\t0.669\tgtfo\n",
            "T\t0.669\tFrance 😡\n",
            "T\t0.669\tgtfo France\n",
            "T\t0.669\t😡 gtfo\n",
            "T\t0.669\tSynset('france.n.02')\n",
            "T\t0.669\tSynset('france.n.01')\n",
            "T\t0.662\tshe\n",
            "T\t0.647\t😡\n",
            "T\t0.575\tu\n",
            "T\t0.557\tSynset('u.n.03')\n",
            "T\t0.557\tSynset('uranium.n.01')\n",
            "T\t0.557\tSynset('uracil.n.01')\n",
            "T\t0.456\tgettin\n",
            "\n",
            "U\t3.130\treeeeeee\n",
            "U\t3.001\treeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
            "U\t1.535\tpeni\n",
            "U\t1.535\tSynset('penis.n.01')\n",
            "U\t0.852\tmori\n",
            "U\t0.852\troodevil\n",
            "U\t0.852\tMORI HER\n",
            "U\t0.852\trooDevil MORI\n",
            "U\t0.834\ther\n",
            "U\t0.673\tnipplesssss\n",
            "U\t0.673\tno nipplesssss\n",
            "U\t0.673\tSynset('nipple.n.02')\n",
            "U\t0.673\tSynset('nipple.n.01')\n",
            "U\t0.655\ttransgenderprid\n",
            "U\t0.655\tda fuk\n",
            "U\t0.655\tTransgenderPride da\n",
            "U\t0.653\tda\n",
            "U\t0.631\tfuk\n",
            "U\t0.625\tcontains_toxic_word\n",
            "U\t0.573\tFUCK LOVE\n",
            "U\t0.568\tSynset('district_attorney.n.01')\n",
            "U\t0.534\t.\n",
            "U\t0.444\tvohiyo\n",
            "U\t0.444\tweeb\n",
            "U\t0.444\tIN VoHiYo\n",
            "\n"
          ]
        }
      ],
      "source": [
        "big_classifier.printWeights(n=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e80DUsSXu7h9"
      },
      "source": [
        "Last, let's look at the individual data points that are most mistaken. It seems like the model picked up on messages containing words written in all caps. It tended to label this either U or T even when the true label was N. I think the most important information to glean from this is the toxic messages that the model tagged as N because those affect viewers more than false positives. We see that stereotypes (\"this music is for men\") are not picked up on (at least sometimes) which makes sense: replacing the word \"men\" with something like \"this music is for everyone\" makes it a non-toxic sentence. Additionally, we can even think of how in this case the vector representation of \"men\" is similar to \"everyone\" because of phrases like man-kind or man-made, to name a few. \n",
        "Further, some additional work could be done to improve the word sense or vector representations for domain specific words such as \"cheater\". This has a greatly negative connotation on Twitch, perhaps even moreso than outside of Twitch. We see that the model missed the toxic message \"sup cheater\" perhaps in part because of this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4uTzwV99pqe"
      },
      "outputs": [],
      "source": [
        "def analyze(classifier):\n",
        "    \n",
        "    probs=classifier.log_reg.predict_proba(classifier.devX)\n",
        "    predicts=classifier.log_reg.predict(classifier.devX)\n",
        "\n",
        "    classes={}\n",
        "    for idx, lab in enumerate(classifier.log_reg.classes_):\n",
        "        classes[lab]=idx\n",
        "\n",
        "    mistakes={}\n",
        "    for i in range(len(probs)):\n",
        "        if predicts[i] != classifier.devY[i]:\n",
        "            predicted_lab_idx=classes[predicts[i]]\n",
        "            mistakes[i]=probs[i][predicted_lab_idx]\n",
        "\n",
        "    frame=[]\n",
        "    sorted_x = sorted(mistakes.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    for k, v in sorted_x:\n",
        "        idd=classifier.devOrig[k][0]\n",
        "        text=classifier.devOrig[k][2]\n",
        "        frame.append([idd, v, classifier.devY[k], predicts[k], text])\n",
        "\n",
        "    df=pd.DataFrame(frame, columns=[\"id\", \"P(predicted class confidence)\", \"Human label\", \"Prediction\", \"Text\"])\n",
        "\n",
        "    with option_context('display.max_colwidth', 400):\n",
        "        display(df.head(n=20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXmRhSuzxaJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "outputId": "d4fc5d2b-04eb-4e8f-dd0f-4deaaed6856c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         id  P(predicted class confidence) Human label Prediction  \\\n",
              "0    510768                       0.998814           T          N   \n",
              "1    549890                       0.991426           T          N   \n",
              "2    559274                       0.986125           T          N   \n",
              "3    119965                       0.973192           U          N   \n",
              "4    869100                       0.949460           T          N   \n",
              "5    248792                       0.911284           N          T   \n",
              "6    664946                       0.889183           N          U   \n",
              "7   1795902                       0.869442           U          N   \n",
              "8    143421                       0.829204           T          N   \n",
              "9    128761                       0.794677           U          N   \n",
              "10  1316412                       0.792708           N          T   \n",
              "11   769432                       0.785832           N          T   \n",
              "12   250689                       0.763470           U          N   \n",
              "13  1445124                       0.723863           U          N   \n",
              "14    58288                       0.640111           N          T   \n",
              "15   227941                       0.633772           N          T   \n",
              "16    29858                       0.578324           U          N   \n",
              "17  2913989                       0.570202           N          U   \n",
              "18   320335                       0.559311           U          T   \n",
              "19  3134489                       0.550527           N          U   \n",
              "\n",
              "                                                                                                     Text  \n",
              "0                    @iwilldominate why do you think an excuse after every death will justify you dying\\n  \n",
              "1                                                                           @norangeee haha shut up lol\\n  \n",
              "2                                                                            Hi cheater @highdistortion\\n  \n",
              "3                                                                       Yeah most of us know better dom\\n  \n",
              "4                                                                                        Sup young buck\\n  \n",
              "5                                                                                 @ChaseShaco luvv u bb\\n  \n",
              "6                                                                           HAMLINZ CARRY mthRee mthRee\\n  \n",
              "7                                                             when nightbot is more famous than you are\\n  \n",
              "8                                                                                 this music is for men\\n  \n",
              "9                                                                       people just dont want the truth\\n  \n",
              "10                                                                     this game look like porn rooDerp\\n  \n",
              "11                                                                                      The more u onow\\n  \n",
              "12                                                @NotSeyfert It's hard to open HS and copy every carD?\\n  \n",
              "13                                                                                                leave\\n  \n",
              "14                 Hashinhashin popping off with his history rants. What can you educate us with IWD???\\n  \n",
              "15                                                                                      ARE U KEEDIN ME\\n  \n",
              "16                                                 they just run in for nothing and all die at the end?\\n  \n",
              "17  YOU ARE NOT READY moon2H YOU ARE NOT READY moon2H YOU ARE NOT READY moon2H YOU ARE NOT READY moon2H\\n  \n",
              "18                                                            HD is clappinh kids left right and centre\\n  \n",
              "19                                                                                  BRAIN POWER POGGERS\\n  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-16c986c7-4949-4707-b693-c04650094612\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>P(predicted class confidence)</th>\n",
              "      <th>Human label</th>\n",
              "      <th>Prediction</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>510768</td>\n",
              "      <td>0.998814</td>\n",
              "      <td>T</td>\n",
              "      <td>N</td>\n",
              "      <td>@iwilldominate why do you think an excuse after every death will justify you dying\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549890</td>\n",
              "      <td>0.991426</td>\n",
              "      <td>T</td>\n",
              "      <td>N</td>\n",
              "      <td>@norangeee haha shut up lol\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>559274</td>\n",
              "      <td>0.986125</td>\n",
              "      <td>T</td>\n",
              "      <td>N</td>\n",
              "      <td>Hi cheater @highdistortion\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>119965</td>\n",
              "      <td>0.973192</td>\n",
              "      <td>U</td>\n",
              "      <td>N</td>\n",
              "      <td>Yeah most of us know better dom\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>869100</td>\n",
              "      <td>0.949460</td>\n",
              "      <td>T</td>\n",
              "      <td>N</td>\n",
              "      <td>Sup young buck\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>248792</td>\n",
              "      <td>0.911284</td>\n",
              "      <td>N</td>\n",
              "      <td>T</td>\n",
              "      <td>@ChaseShaco luvv u bb\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>664946</td>\n",
              "      <td>0.889183</td>\n",
              "      <td>N</td>\n",
              "      <td>U</td>\n",
              "      <td>HAMLINZ CARRY mthRee mthRee\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1795902</td>\n",
              "      <td>0.869442</td>\n",
              "      <td>U</td>\n",
              "      <td>N</td>\n",
              "      <td>when nightbot is more famous than you are\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>143421</td>\n",
              "      <td>0.829204</td>\n",
              "      <td>T</td>\n",
              "      <td>N</td>\n",
              "      <td>this music is for men\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>128761</td>\n",
              "      <td>0.794677</td>\n",
              "      <td>U</td>\n",
              "      <td>N</td>\n",
              "      <td>people just dont want the truth\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1316412</td>\n",
              "      <td>0.792708</td>\n",
              "      <td>N</td>\n",
              "      <td>T</td>\n",
              "      <td>this game look like porn rooDerp\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>769432</td>\n",
              "      <td>0.785832</td>\n",
              "      <td>N</td>\n",
              "      <td>T</td>\n",
              "      <td>The more u onow\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>250689</td>\n",
              "      <td>0.763470</td>\n",
              "      <td>U</td>\n",
              "      <td>N</td>\n",
              "      <td>@NotSeyfert It's hard to open HS and copy every carD?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1445124</td>\n",
              "      <td>0.723863</td>\n",
              "      <td>U</td>\n",
              "      <td>N</td>\n",
              "      <td>leave\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>58288</td>\n",
              "      <td>0.640111</td>\n",
              "      <td>N</td>\n",
              "      <td>T</td>\n",
              "      <td>Hashinhashin popping off with his history rants. What can you educate us with IWD???\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>227941</td>\n",
              "      <td>0.633772</td>\n",
              "      <td>N</td>\n",
              "      <td>T</td>\n",
              "      <td>ARE U KEEDIN ME\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>29858</td>\n",
              "      <td>0.578324</td>\n",
              "      <td>U</td>\n",
              "      <td>N</td>\n",
              "      <td>they just run in for nothing and all die at the end?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2913989</td>\n",
              "      <td>0.570202</td>\n",
              "      <td>N</td>\n",
              "      <td>U</td>\n",
              "      <td>YOU ARE NOT READY moon2H YOU ARE NOT READY moon2H YOU ARE NOT READY moon2H YOU ARE NOT READY moon2H\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>320335</td>\n",
              "      <td>0.559311</td>\n",
              "      <td>U</td>\n",
              "      <td>T</td>\n",
              "      <td>HD is clappinh kids left right and centre\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3134489</td>\n",
              "      <td>0.550527</td>\n",
              "      <td>N</td>\n",
              "      <td>U</td>\n",
              "      <td>BRAIN POWER POGGERS\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16c986c7-4949-4707-b693-c04650094612')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-16c986c7-4949-4707-b693-c04650094612 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-16c986c7-4949-4707-b693-c04650094612');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "analyze(big_classifier)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Twitch Classifier v2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}